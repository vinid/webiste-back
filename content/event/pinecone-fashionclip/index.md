---
title: "Talk: Domain-specific multi-modal ML with CLIP"

event: Invited Talk

location: Pinecone

summary: Building Domain-specific contrastive models for language and image.


abstract: "In this talk we explore a domain-specific application CLIP: FashionCLIP. We describe a contrastive models trained on 800K fashion caption-image pairs and show its usefulness in applied tasks."

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: "2022-09-29T13:00:00Z"
date_end: "2022-09-29T15:00:00Z"
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: "2022-09-29T00:00:00Z"

authors: ["Federico Bianchi"]
tags: []

# Is this a featured talk? (true/false)
featured: false

image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/kRNZiGKtz48)'
  focal_point: Right

links:
- icon: twitter
  icon_pack: fab
  name: Follow
  url: https://twitter.com/federicobianchy
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.


# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.

---
