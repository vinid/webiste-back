---
title: "Teaching CLIP Some Fashion"
subtitle: "Teaching CLIP Some Fashion"

# Summary for listings and search engines
summary: "This article describes how to implement CLIP to support a domain-specific fashion use case"

# Link this post with a project
projects: [Vision and Language Models]

# Date published
date: "2023-03-11T00:00:00Z"

# Date updated
lastmod: "2023-03-11T00:00:00Z"

# Is this an unpublished draft?
draft: false

# Show this page in the Featured widget?
featured: true

# Featured image
# Place an image named `featured.jpg/png` in this page's folder and customize its options here.
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/hGV2TfOh0ns)'
  focal_point: ""
  placement: 1
  preview_only: false

authors:
- Federico Bianchi


tags:
- Academic
- AI

categories:
- Vision and Language Models

links:
 - name: Read on Medium
   url: https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3

---

## Overview

This is a short blog post describing FashionCLIP. If you are a data scientist you probably have to deal with both images and text. However, your data will be very specific to your domain, and standard models might not work well. This post explains how domain-specific vision and language models can be used in a domain-specific setting and why using them can be a promising way to create a search engine or a (zero-shot) classifier.

[Read More](https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3)
